# AgentFlow 开发计划：基于信令服务器的P2P直连架构
## 1. 项目概况
本项目旨在构建一个高性能的分布式监控系统，利用低成本公网 Pod 作为“信令索引”实现节点发现，各高性能 Master 节点之间通过 gRPC 建立直连传输数据，完全避开公网带宽瓶颈。
---
## 2. 技术栈定义
*   **信令服务器**: Rust + Axum (HTTP), 内存存储或 Redis
*   **Master 节点**: Rust + Tokio + Tonic (gRPC)
*   **前端大屏**: React + TypeScript + WebSocket (连接本地 Master)
*   **网络协议**: HTTP (信令), gRPC (P2P数据流), mTLS (安全认证)
---
## 3. 模块化任务分解
此计划按模块划分，不同模块的任务可以并行开发。
### 模块 A：公网信令服务器
**目标**：提供极轻量的节点注册、发现和心跳保活服务。
**优先级**：最高（阻塞其他模块的联调）
*   **A-1 [API 设计]**: 定义 REST API 接口 (`/register`, `/list`, `/heartbeat`) 及数据结构 JSON Schema。
*   **A-2 [基础服务搭建]**: 初始化 Rust/Axum 项目，配置 CORS 和日志中间件。
*   **A-3 [存储层实现]**: 实现线程安全的内存存储 `HashMap<NodeId, NodeInfo>`，支持读写锁。
*   **A-4 [业务逻辑开发]**: 实现注册、列表查询接口；实现心跳超时检测逻辑（自动剔除离线节点）。
*   **A-5 [部署脚本]**: 编写 Dockerfile，构建超小体积镜像，部署至公网 Pod。
### 模块 B：P2P 网络通信层
**目标**：实现 Master 节点间的自动发现、连接建立、断线重连及底层加密。
**优先级**：最高
*   **B-1 [Protobuf 定义]**: 编写 `p2p_mesh.proto`，定义 `P2PMesh` Service (双向流) 及消息类型。
*   **B-2 [代码生成与封装]**: 使用 Tonic 生成 Rust 代码，封装客户端和服务端的基础类。
*   **B-3 [服务端监听]**: 在 Master 启动时开启 gRPC 监听服务（默认端口 50051），处理入站连接请求。
*   **B-4 [客户端发现]**: 实现 `DiscoveryClient`，定期向信令服务器拉取 Peer 列表。
*   **B-5 [连接管理器]**: 实现 `ConnectionManager`，维护一个 `HashMap<NodeId, Channel>`。逻辑包括：
    *   识别新 Peer 并发起连接。
    *   维护连接池，处理连接断开后的自动重连。
*   **B-6 [安全加密]**: 配置 Rustls，为 P2P 连接添加 mTLS 认证（确保只有授权 Master 才能连入）。
### 模块 C：分布式状态同步
**目标**：实现状态变更的实时广播和全网状态的汇聚。
**优先级**：中
*   **C-1 [事件定义]**: 定义需要广播的事件类型（NodeStatusUpdate, TaskLog, Alert）。
*   **C-2 [广播器]**: 实现逻辑：当本地状态变更时，遍历 `ConnectionManager` 中的所有活跃连接，通过 gRPC Stream 发送事件。
*   **C-3 [状态汇聚器]**: 实现逻辑：接收来自 Peer 的事件，更新本地的 `RemoteNodeStore` 缓存。
*   **C-4 [全量同步机制]**: 实现新节点加入时的“握手同步”——向对方请求当前全量状态快照，避免仅依靠增量消息丢失数据。
### 模块 D：监控大屏与接口
**目标**：前端展示全网聚合状态，并提供控制入口。
**优先级**：中
*   **D-1 [本地 WebSocket 服务]**: 在 Master 上开发 WebSocket 服务，供前端连接。
*   **D-2 [聚合查询接口]**: Master 提供一个 HTTP API `/api/cluster/status`，返回 `{ local: LocalState, remote: Vec<RemoteState> }`。
*   **D-3 [前端拓扑图]**: 使用 React Flow 或 ECharts 开发节点拓扑图，展示 Master 间的连接关系和状态。
*   **D-4 [实时数据流]**: 前端通过 WebSocket 订阅 Master 的状态变更，Master 收到 Peer 的更新后，通过 WebSocket 推送给前端。
*   **D-5 [指令路由]**: 前端发送“控制指令”到本地 Master，Master 解析如果目标是其他 Peer，则通过 P2P 连接转发。
### 模块 E：基础设施与网络配置
**目标**：解决 NAT 穿透和部署问题。
**优先级**：关键（非代码类）
*   **E-1 [NAT/端口映射指南]**: 编写文档，指导如何配置家庭/云服务器的路由器端口映射，确保 Master 的 gRPC 端口可被公网访问。
*   **E-2 [STUN 辅助 (可选)]**: 如果直连困难，集成简单的 STUN 客户端帮助节点发现自己的公网 IP。
*   **E-3 [容器化部署]**: 编写 `docker-compose.yml`，一键启动 Master 节点（自动挂载配置和证书）。
---
## 4. 实施路线图
### 第 1 周：骨架搭建 (并行执行)
*   **Team A**: 完成信令服务器开发并部署上线。
*   **Team B**: 完成 Proto 定义，生成代码，跑通 Master 节点的基础监听。
*   **Team E**: 确认至少两个测试环境的网络可达性（配置好端口映射）。
### 第 2 周：网络互通 (联调)
*   **Team B**: 完成信令客户端逻辑，实现 Master A 自动发现并连接到 Master B。
*   **Team C**: 实现基础的心跳包通过 P2P 传输，验证连接稳定性。
*   **Milestone**: 两个 Master 节点能互相看到对方在线，且连接不经过公网 Pod。
### 第 3 周：数据流转与同步
*   **Team C**: 实现状态广播。Master A 修改任务状态，Master B 立即收到更新。
*   **Team D**: 前端连接 Master A，要求能看到 A 和 B 的聚合数据。
*   **Milestone**: 监控大屏能实时展示多节点状态，刷新延迟 < 500ms。
### 第 4 周：控制、优化与部署
*   **Team D**: 实现指令下发（大屏控制 Master B 的任务）。
*   **Team B**: 添加 mTLS 安全认证，防止未授权连接。
*   **Team E**: 优化 Docker 镜像，编写运维文档。
---
## 5. 关键验收指标
1.  **公网 Pod CPU 占用**: 始终 < 5%（即使 10+ 节点在线）。
2.  **P2P 连接数**: N 个节点形成全网状连接。
3.  **数据延迟**: Master A 状态变化到 Master B 感知 < 200ms (取决于物理网络)。
4.  **容错性**: 公网 Pod 挂掉，已直连的 Master 间通信不受影响；公网 Pod 恢复，新 Master 能加入。
## 6. 风险与对策
| 风险点                       | 应对策略                                                                                                  |
| :--------------------------- | :-------------------------------------------------------------------------------------------------------- |
| **家庭网络 NAT 穿透失败**    | 强制要求配置端口映射 (DMZ 或 Port Forwarding)；提供检测工具。                                             |
| **P2P 连接数过多导致负载高** | 实现连接数限制（如最多直连 5 个邻居），非直连数据通过邻居转发（虽然初期规划是全连接，但需保留优化余地）。 |
| **IP 变动导致连接中断**      | Master 定期向信令服务器更新 IP；连接断开时自动去信令服务器查询最新 IP 重连。                              |
